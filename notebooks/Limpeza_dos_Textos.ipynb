{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando as bibliotecas\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classe\n",
       "Sentenca                 9511\n",
       "Peticao Intermediaria    9178\n",
       "Prazo Parte Contraria    5293\n",
       "Audiencia                4378\n",
       "Acordao                  4238\n",
       "Pauta de Julgamento      3221\n",
       "Contrarrazoes            3162\n",
       "Outros                   2897\n",
       "Conclusao                1988\n",
       "Contestacao              1192\n",
       "Liminar                   422\n",
       "Decisao                    98\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lendo o dataset tratado\n",
    "df = pd.read_excel('C:/Users/atmaz1/Desktop/AED - Classificação de Publicações/bdpublicacaofinal.xlsx')\n",
    "df['classe'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\atmaz1\\appdata\\roaming\\python\\python312\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\atmaz1\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\atmaz1\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\atmaz1\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (2024.5.10)\n",
      "Requirement already satisfied: tqdm in c:\\users\\atmaz1\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\atmaz1\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\atmaz1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\atmaz1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Carregando Bibliotecas do nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classe\n",
      "Acordao      4000\n",
      "Audiencia    4000\n",
      "Sentenca     4000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Classes desejadas e número de linhas por classe\n",
    "classes_desejadas = ['Acordao', 'Audiencia', 'Sentenca']\n",
    "num_linhas_por_classe = 4000\n",
    "\n",
    "# Lista para armazenar amostras de cada classe\n",
    "amostras_por_classe = []\n",
    "\n",
    "# Para cada classe desejada, selecionar aleatoriamente 'num_linhas_por_classe' amostras\n",
    "for classe in classes_desejadas:\n",
    "    df_classe = df[df['classe'] == classe]\n",
    "    \n",
    "    # Verificar se a classe tem linhas suficientes\n",
    "    if len(df_classe) < num_linhas_por_classe:\n",
    "        raise ValueError(f\"A classe {classe} não tem linhas suficientes. Tem apenas {len(df_classe)} linhas.\")\n",
    "    \n",
    "    # Selecionar aleatoriamente 'num_linhas_por_classe' amostras\n",
    "    amostras_classe = df_classe.sample(n=num_linhas_por_classe, random_state=42)\n",
    "    amostras_por_classe.append(amostras_classe)\n",
    "\n",
    "# Combinar todas as amostras em um único DataFrame\n",
    "df_amostras = pd.concat(amostras_por_classe)\n",
    "\n",
    "# Verificar a distribuição das classes nas amostras selecionadas\n",
    "print(df_amostras['classe'].value_counts())\n",
    "\n",
    "# Salvar as amostras selecionadas em um novo arquivo CSV, se necessário\n",
    "df_amostras.to_csv('amostras_selecionadas.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('amostras_selecionadas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\atmaz1\\AppData\\Local\\Temp\\2\\ipykernel_6304\\3529497774.py:3: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  x = re.sub('[,\\.!?:()\"]', '', x) # remoção de caracteres especiais\n",
      "C:\\Users\\atmaz1\\AppData\\Local\\Temp\\2\\ipykernel_6304\\3529497774.py:5: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  x = re.sub('http\\S+', ' ', x)     # remoção do https\n",
      "C:\\Users\\atmaz1\\AppData\\Local\\Temp\\2\\ipykernel_6304\\3529497774.py:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  x = re.sub('\\s+', ' ', x)            # remoção de espaços em branco duplicados\n"
     ]
    }
   ],
   "source": [
    "#Limpeza no texto\n",
    "def process(x):\n",
    "    x = re.sub('[,\\.!?:()\"]', '', x) # remoção de caracteres especiais\n",
    "    x = re.sub('<.*?>', ' ', x)      # remoção de conteudos entre <>\n",
    "    x = re.sub('http\\S+', ' ', x)     # remoção do https\n",
    "    x = re.sub('[^a-zA-Z0-9]', ' ', x)  # remoção de qualquer caractere que não seja uma letra ou números entre 0 e 9\n",
    "    x = re.sub('\\s+', ' ', x)            # remoção de espaços em branco duplicados\n",
    "    return x.lower().strip()\n",
    "\n",
    "dataframe['texto'] = dataframe['descricao_classificacoes'].apply(lambda x: process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.drop('descricao_classificacoes', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seria', 'tivemos', 'as', 'deles', 'é', 'temos', 'mesmo', 'fossem', 'minha', 'tivesse', 'dos', 'havemos', 'isto', 'aquele', 'tuas', 'essas', 'terei', 'um', 'estamos', 'delas', 'te', 'nas', 'quem', 'entre', 'teria', 'aqueles', 'era', 'haja', 'tivessem', 'teremos', 'tem', 'em', 'tenhamos', 'aquilo', 'me', 'tivéramos', 'teve', 'a', 'hão', 'estivermos', 'isso', 'estiverem', 'lhes', 'houverá', 'serei', 'houve', 'os', 'estão', 'aos', 'elas', 'houvemos', 'estiver', 'lhe', 'fora', 'estejam', 'pelo', 'sou', 'suas', 'estar', 'tenho', 'depois', 'quando', 'eram', 'houvéramos', 'houverem', 'tiverem', 'no', 'à', 'teríamos', 'nossa', 'ao', 'tu', 'seriam', 'tinham', 'estive', 'por', 'terão', 'ser', 'dela', 'estivera', 'houveremos', 'tínhamos', 'como', 'houvera', 'houverão', 'se', 'houvéssemos', 'sejamos', 'houveríamos', 'aquelas', 'às', 'esteve', 'nos', 'das', 'até', 'e', 'já', 'forem', 'houveriam', 'o', 'estava', 'teu', 'estivéssemos', 'terá', 'esse', 'teus', 'tivera', 'que', 'tivéssemos', 'fomos', 'minhas', 'tém', 'hajam', 'fôssemos', 'houverei', 'seus', 'pela', 'teriam', 'qual', 'seu', 'serão', 'sejam', 'da', 'esses', 'com', 'estivemos', 'estavam', 'sua', 'esteja', 'num', 'há', 'você', 'nós', 'nossas', 'de', 'estou', 'estejamos', 'ele', 'não', 'estivessem', 'houveram', 'seremos', 'também', 'tiveram', 'mais', 'éramos', 'houver', 'tenha', 'nosso', 'estávamos', 'essa', 'houveria', 'estes', 'estivéramos', 'eu', 'tivermos', 'houvessem', 'tenham', 'numa', 'estiveram', 'pelas', 'ela', 'meu', 'na', 'tive', 'foi', 'vocês', 'somos', 'do', 'seja', 'está', 'seríamos', 'meus', 'fôramos', 'muito', 'estas', 'hajamos', 'for', 'foram', 'nem', 'só', 'este', 'hei', 'para', 'nossos', 'ou', 'aquela', 'são', 'mas', 'eles', 'formos', 'houvesse', 'tinha', 'esta', 'tiver', 'fosse', 'dele', 'haver', 'será', 'fui', 'tua', 'estivesse', 'vos', 'houvermos', 'pelos', 'uma', 'sem'}\n"
     ]
    }
   ],
   "source": [
    "# Remoção de Stop Words\n",
    "sw_set = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "print(sw_set)\n",
    "\n",
    "def sw_remove(x):\n",
    "    words = nltk.tokenize.word_tokenize(x)\n",
    "    filtered_list = [word for word in words if word not in sw_set]\n",
    "    return ' '.join(filtered_list)\n",
    "\n",
    "dataframe['texto_preprocessing'] = dataframe['texto'].apply(lambda x: sw_remove(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mtrain_txt.shape:\u001b[0m (10800,)\n",
      "\u001b[1mtest_txt.shape:\u001b[0m (1200,)\n",
      "\u001b[1mtrain_class.shape:\u001b[0m (10800,)\n",
      "\u001b[1mtest_class.shape:\u001b[0m (1200,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Separando os Dados de Treino e Teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataframe['texto_preprocessing'], dataframe['classe'], test_size=0.1, random_state=1)\n",
    "\n",
    "print('\\033[1m' + 'train_txt.shape:' + '\\033[0m', X_train.shape)\n",
    "print('\\033[1m' + 'test_txt.shape:' + '\\033[0m', X_test.shape)\n",
    "print('\\033[1m' + 'train_class.shape:' + '\\033[0m', y_train.shape)\n",
    "print('\\033[1m' + 'test_class.shape:' + '\\033[0m', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pacotes para rede neural\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDictionary size:\u001b[0m 1000\n",
      "\u001b[1mLength of the tokenizer index:\u001b[0m 7930\n",
      "\u001b[1mNumber of documents the tokenizer was trained on:\u001b[0m 12000 \n",
      "\n",
      "\u001b[1mFirst 20 entries of the tokenizer index:\u001b[0m\n",
      "('x000d', 1) ('ncia', 2) ('2024', 3) ('exposto', 4) ('recurso', 5) ('audi', 6) ('provimento', 7) ('julgo', 8) ('s', 9) ('art', 10) ('dia', 11) ('c', 12) ('processo', 13) ('designada', 14) ('ante', 15) ('concilia', 16) ('termos', 17) ('conhecer', 18) ('parte', 19) ('cpc', 20)\n"
     ]
    }
   ],
   "source": [
    "#Treinando o Tokenizador que irá transformar Palavras em Vetores\n",
    "top_words = 1000\n",
    "tokenizer = Tokenizer(num_words=top_words)\n",
    "tokenizer.fit_on_texts(dataframe['texto_preprocessing'])\n",
    "\n",
    "print('\\033[1m' + 'Dictionary size:' + '\\033[0m', top_words)\n",
    "print('\\033[1m' + 'Length of the tokenizer index:' + '\\033[0m', len(tokenizer.word_index))\n",
    "print('\\033[1m' + 'Number of documents the tokenizer was trained on:' + '\\033[0m', tokenizer.document_count, '\\n')\n",
    "print('\\033[1m' + 'First 20 entries of the tokenizer index:' + '\\033[0m')\n",
    "print(*list(tokenizer.word_index.items())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mX_train.shape:\u001b[0m (10800,)\n",
      "\u001b[1mX_test.shape:\u001b[0m (1200,) \n",
      "\n",
      "\u001b[1mReview #2340 after padding:\u001b[0m\n",
      " unanimidade votos conhecer embargos declara\n"
     ]
    }
   ],
   "source": [
    "# Padronizando Tamanho dos textos\n",
    "try:\n",
    "  max_text_length = 100\n",
    "  X_train = pad_sequences(X_train, maxlen=max_text_length)\n",
    "  X_test = pad_sequences(X_test, maxlen=max_text_length)\n",
    "except:\n",
    "  pass\n",
    "\n",
    "print('\\033[1m' + 'X_train.shape:' + '\\033[0m', X_train.shape)\n",
    "print('\\033[1m' + 'X_test.shape:' + '\\033[0m', X_test.shape, '\\n')\n",
    "\n",
    "# Printing an example of review after padding\n",
    "\n",
    "idx_pad = random.randint(0, len(X_train)-1)\n",
    "print('\\033[1m' + 'Review #%d after padding:' %idx_pad + '\\033[0m' + '\\n', X_train[idx_pad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Criar o modelo\n",
    "model = Sequential()\n",
    "\n",
    "# Adicionar camadas\n",
    "model.add(Embedding(input_dim=100, output_dim=100))\n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Verificar o sumário do modelo\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
